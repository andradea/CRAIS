{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, lit\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec, Word2VecModel\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.driver.memory', '3g')\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '3g')\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"word2vec\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"vector\")\n",
    "remover = StopWordsRemover()\n",
    "remover.setInputCol(\"vector\")\n",
    "remover.setOutputCol(\"vector_no_stopw\")\n",
    "stopwords = remover.getStopWords()\n",
    "stemmer = PorterStemmer()\n",
    "stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n",
    "\n",
    "def word_to_index(df):\n",
    "    df = df.sample(False, 0.1, 42)\n",
    "    df = df.selectExpr(\"value as text\")\n",
    "    df = tokenize_df(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def tokenize_df(df):    \n",
    "    df = df.select(clean_text(col(\"text\")).alias(\"text\"))\n",
    "    df = tokenizer.transform(df).select(\"vector\")\n",
    "    df = remover.transform(df).select(\"vector_no_stopw\")\n",
    "    df = (df\n",
    "        .withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n",
    "        .select(\"vector_stemmed\")\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def clean_text(c):\n",
    "    c = lower(c)\n",
    "    c = regexp_replace(c, \"^rt \", \"\")\n",
    "    c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n",
    "    c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "    c = regexp_replace(c, \"[0-9]\", \"\")\n",
    "  #c = split(c, \"\\\\s+\") tokenization...\n",
    "    return c\n",
    "\n",
    "\n",
    "def stem(in_vec):\n",
    "    out_vec = []\n",
    "    for t in in_vec:\n",
    "        t_stem = stemmer.stem(t)\n",
    "        if len(t_stem) > 2:\n",
    "            out_vec.append(t_stem)       \n",
    "    return out_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      vector_stemmed|\n",
      "+--------------------+\n",
      "|[charlevoix, detr...|\n",
      "|[mhta, presid, ce...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/andradea/Documents/languages/en_US/'\n",
    "\n",
    "news = spark.read.text(path + 'en_US.news.txt')\n",
    "news = word_to_index(news)\n",
    "# news = news.withColumn('y', lit(1))\n",
    "\n",
    "news.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      vector_stemmed|\n",
      "+--------------------+\n",
      "|              [bear]|\n",
      "|[winter, time, sl...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs = spark.read.text(path + 'en_US.blogs.txt')\n",
    "blogs = word_to_index(blogs)\n",
    "# blogs = blogs.withColumn('y', lit(0))\n",
    "\n",
    "blogs.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = news.union(blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/andradea/Documents/languages/en_US/word_index.pkl', 'rb') as f:\n",
    "    word_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charlevoix', 'detroit']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = news.take(1)\n",
    "row[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_index(sentence, word_index, max_length = 100):\n",
    "    indexed = np.zeros(max_length)\n",
    "    for i, word in enumerate(sentence):\n",
    "        if i < max_length:\n",
    "            if word in (word_index.keys()):\n",
    "                indexed[i] = word_index[word]\n",
    "    return indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_to_index = [sentence_to_index(row[0], word_index) for row in news.collect()]\n",
    "news_to_index = np.asarray(news_to_index)\n",
    "y_news = np.repeat(1, len(news_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_to_index = [sentence_to_index(row[0], word_index) for row in blogs.collect()]\n",
    "blogs_to_index = np.asarray(blogs_to_index)\n",
    "y_blogs = np.repeat(0, len(blogs_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101193, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(news_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([news_to_index, blogs_to_index])\n",
    "Y = np.hstack([y_news, y_blogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/andradea/Documents/languages/en_US/sentences.pkl', 'wb') as f:\n",
    "    pickle.dump((X, Y), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
