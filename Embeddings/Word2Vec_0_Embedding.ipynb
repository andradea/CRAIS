{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Creation\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark as ps\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.driver.memory', '3g')\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '3g')\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"word2vec\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en_US.blogs.txt', 'en_US.news.txt']\n",
      "total number of lines in df: 1909531\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/andradea/Documents/languages/en_US/'\n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if f.startswith('en') == True and 'twitter' not in f]\n",
    "print(files)\n",
    "\n",
    "df = spark.createDataFrame([['']])\n",
    "\n",
    "for file in files:\n",
    "    df = df.union(spark.read.text(path + file))\n",
    "    \n",
    "# df = df.sample(withReplacement=False, fraction=0.75, seed=42)\n",
    "\n",
    "old_col = df.schema.names[0]\n",
    "df = df.selectExpr(old_col + ' as text')\n",
    "\n",
    "print('total number of lines in df:', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"vector\")\n",
    "remover = StopWordsRemover()\n",
    "remover.setInputCol(\"vector\")\n",
    "remover.setOutputCol(\"vector_no_stopw\")\n",
    "stopwords = remover.getStopWords()\n",
    "stemmer = PorterStemmer()\n",
    "stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n",
    "\n",
    "def tokenize_df(df):    \n",
    "    df = df.select(clean_text(col(\"text\")).alias(\"text\"))\n",
    "    df = tokenizer.transform(df).select(\"vector\")\n",
    "    df = remover.transform(df).select(\"vector_no_stopw\")\n",
    "    df = (df\n",
    "        .withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n",
    "        .select(\"vector_stemmed\")\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def clean_text(c):\n",
    "    c = lower(c)\n",
    "    c = regexp_replace(c, \"^rt \", \"\")\n",
    "    c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n",
    "    c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "    c = regexp_replace(c, \"[0-9]\", \"\")\n",
    "  #c = split(c, \"\\\\s+\") tokenization...\n",
    "    return c\n",
    "\n",
    "\n",
    "def stem(in_vec):\n",
    "    out_vec = []\n",
    "    for t in in_vec:\n",
    "        t_stem = stemmer.stem(t)\n",
    "        if len(t_stem) > 2:\n",
    "            out_vec.append(t_stem)       \n",
    "    return out_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|         word|              vector|\n",
      "+-------------+--------------------+\n",
      "|     nijinski|[-0.0625451356172...|\n",
      "|       ciresi|[-0.0682697072625...|\n",
      "|         koel|[0.00719995377585...|\n",
      "|        doili|[23.2668304443359...|\n",
      "|         onam|[0.17379367351531...|\n",
      "|      rahmani|[-0.0066115041263...|\n",
      "|    budgetwis|[-0.0064925411716...|\n",
      "|        dredd|[-1.5024898052215...|\n",
      "|       gaiden|[0.02443256787955...|\n",
      "|     autofocu|[-0.2609729170799...|\n",
      "|     quotient|[-1.1691495180130...|\n",
      "|   hirschhorn|[-0.0670437887310...|\n",
      "|     clarissa|[-0.1497923582792...|\n",
      "|     incident|[7.36339569091796...|\n",
      "|      holsman|[-0.0202226024121...|\n",
      "|meteorologist|[-6.8392672538757...|\n",
      "|       gaslit|[-0.0221786983311...|\n",
      "|      aikenit|[0.09228715300559...|\n",
      "|  seventhgrad|[-4.1771936416625...|\n",
      "|        hetch|[-1.6678887605667...|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=150, minCount=5, numPartitions=400, stepSize=0.025, maxIter=1, seed=42, \n",
    "                    windowSize=5, maxSentenceLength=1000, inputCol=\"vector_stemmed\", outputCol=\"result\")\n",
    "\n",
    "df = tokenize_df(df)\n",
    "model = word2Vec.fit(df)\n",
    "model.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path + \"/word2vec-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
