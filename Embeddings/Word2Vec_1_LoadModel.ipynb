{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec, Word2VecModel\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.driver.memory', '3g')\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '3g')\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"word2vec\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"vector\")\n",
    "remover = StopWordsRemover()\n",
    "remover.setInputCol(\"vector\")\n",
    "remover.setOutputCol(\"vector_no_stopw\")\n",
    "stopwords = remover.getStopWords()\n",
    "stemmer = PorterStemmer()\n",
    "stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n",
    "\n",
    "def tokenize_df(df):    \n",
    "    df = df.select(clean_text(col(\"text\")).alias(\"text\"))\n",
    "    df = tokenizer.transform(df).select(\"vector\")\n",
    "    df = remover.transform(df).select(\"vector_no_stopw\")\n",
    "    df = (df\n",
    "        .withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))\n",
    "        .select(\"vector_stemmed\")\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def clean_text(c):\n",
    "  c = lower(c)\n",
    "  c = regexp_replace(c, \"^rt \", \"\")\n",
    "  c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n",
    "  c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "  #c = split(c, \"\\\\s+\") tokenization...\n",
    "  return c\n",
    "\n",
    "\n",
    "def stem(in_vec):\n",
    "    out_vec = []\n",
    "    for t in in_vec:\n",
    "        t_stem = stemmer.stem(t)\n",
    "        if len(t_stem) > 2:\n",
    "            out_vec.append(t_stem)       \n",
    "    return out_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\", ),\n",
    "    (\"I wish Java could use case classes\", ),\n",
    "    (\"Logistic regression models are neat\", )\n",
    "], [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vector_stemmed=['heard', 'spark']),\n",
       " Row(vector_stemmed=['wish', 'java', 'use', 'case', 'class'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filt = tokenize_df(df)\n",
    "df_filt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|         word|              vector|\n",
      "+-------------+--------------------+\n",
      "|     nijinski|[-0.0625451356172...|\n",
      "|       ciresi|[-0.0682697072625...|\n",
      "|         koel|[0.00719995377585...|\n",
      "|        doili|[23.2668304443359...|\n",
      "|         onam|[0.17379367351531...|\n",
      "|      rahmani|[-0.0066115041263...|\n",
      "|    budgetwis|[-0.0064925411716...|\n",
      "|        dredd|[-1.5024898052215...|\n",
      "|       gaiden|[0.02443256787955...|\n",
      "|     autofocu|[-0.2609729170799...|\n",
      "|     quotient|[-1.1691495180130...|\n",
      "|   hirschhorn|[-0.0670437887310...|\n",
      "|     clarissa|[-0.1497923582792...|\n",
      "|     incident|[7.36339569091796...|\n",
      "|      holsman|[-0.0202226024121...|\n",
      "|meteorologist|[-6.8392672538757...|\n",
      "|       gaslit|[-0.0221786983311...|\n",
      "|      aikenit|[0.09228715300559...|\n",
      "|  seventhgrad|[-4.1771936416625...|\n",
      "|        hetch|[-1.6678887605667...|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/andradea/Documents/languages/en_US/'\n",
    "model = Word2VecModel.load(path + \"/word2vec-model\")\n",
    "model.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = model.getVectors().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nijinski</td>\n",
       "      <td>[-0.06254513561725616, -0.04033434018492699, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ciresi</td>\n",
       "      <td>[-0.06826970726251602, -0.11408036947250366, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>koel</td>\n",
       "      <td>[0.007199953775852919, -0.04827534779906273, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doili</td>\n",
       "      <td>[23.266830444335938, -64.8271713256836, 7.6823...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>onam</td>\n",
       "      <td>[0.17379367351531982, -0.17130979895591736, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word                                             vector\n",
       "0  nijinski  [-0.06254513561725616, -0.04033434018492699, 0...\n",
       "1    ciresi  [-0.06826970726251602, -0.11408036947250366, 0...\n",
       "2      koel  [0.007199953775852919, -0.04827534779906273, -...\n",
       "3     doili  [23.266830444335938, -64.8271713256836, 7.6823...\n",
       "4      onam  [0.17379367351531982, -0.17130979895591736, -0..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index\n",
    "word_index = {}\n",
    "for i in range(word_vector.word.count()):\n",
    "    word_index[word_vector.iloc[i, 0]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_index\n",
    "embeddings_index = {}\n",
    "for i in range(word_vector.vector.count()):\n",
    "    embeddings_index[i] = word_vector.iloc[i, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, len(embeddings_index[0])))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(i)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/Users/andradea/Documents/languages/en_US/embeddings.txt', embedding_matrix, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/andradea/Documents/languages/en_US/word_index.pkl', 'wb') as f:\n",
    "    pickle.dump(word_index, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/andradea/Documents/languages/en_US/embeddings_index.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_index, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
