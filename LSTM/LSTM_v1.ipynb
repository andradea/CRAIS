{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/andradea/Documents/languages/en_US/'\n",
    "N = 10000\n",
    "tweets = pd.read_csv(path + 'en_US.twitter.txt', header=None, delimiter='\\n', nrows=N)\n",
    "blogs = pd.read_csv(path + 'en_US.blogs.txt', header=None, delimiter='\\n', nrows=N)\n",
    "news = pd.read_csv(path + 'en_US.news.txt', header=None, delimiter='\\n', nrows=N)\n",
    "papers = pd.read_csv(path + 'papers.csv', usecols=['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[0]\n",
    "blogs = blogs[0]\n",
    "news = news[0]\n",
    "papers = [p for p in papers['abstract'] if p != 'Abstract Missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "vec_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_vectors_web_lg', disable=['parser', 'tagger', 'ner'])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of words per sentence: 9.745479400981793\n"
     ]
    }
   ],
   "source": [
    "sen_len = []\n",
    "tweet_sen = 0\n",
    "for tweet in tweets:\n",
    "    tokens  = nlp(tweet)\n",
    "    for sentence in tokens.sents:\n",
    "        sen_len.append(len(sentence))\n",
    "        tweet_sen += 1\n",
    "print('average number of words per sentence:', np.mean(sen_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of words per sentence: 18.44062744513581\n"
     ]
    }
   ],
   "source": [
    "sen_len = []\n",
    "blog_sen = 0\n",
    "for blog in blogs:\n",
    "    tokens  = nlp(blog)\n",
    "    for sentence in tokens.sents:\n",
    "        sen_len.append(len(sentence))\n",
    "        blog_sen += 1\n",
    "print('average number of words per sentence:', np.mean(sen_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of words per sentence: 21.204551305153682\n"
     ]
    }
   ],
   "source": [
    "sen_len = []\n",
    "news_sen = 0\n",
    "for new in news:\n",
    "    tokens  = nlp(new)\n",
    "    for sentence in tokens.sents:\n",
    "        sen_len.append(len(sentence))\n",
    "        news_sen += 1\n",
    "print('average number of words per sentence:', np.mean(sen_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of words per sentence: 27.087230287960214\n"
     ]
    }
   ],
   "source": [
    "sen_len = []\n",
    "paper_sen = 0\n",
    "for paper in papers:\n",
    "    tokens  = nlp(paper)\n",
    "    for sentence in tokens.sents:\n",
    "        sen_len.append(len(sentence))\n",
    "        paper_sen += 1\n",
    "print('average number of words per sentence:', np.mean(sen_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweets sentences: 16093\n",
      "total blogs sentences: 26839\n",
      "total news sentences: 19423\n",
      "total papers sentences: 24934\n"
     ]
    }
   ],
   "source": [
    "print('total tweets sentences:', tweet_sen)\n",
    "print('total blogs sentences:', blog_sen)\n",
    "print('total news sentences:', news_sen)\n",
    "print('total papers sentences:', paper_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.534331697011123\n",
      "0.02486538720908001\n"
     ]
    }
   ],
   "source": [
    "tweet_x = np.zeros((tweet_sen, max_len, vec_size))\n",
    "i = 0\n",
    "z = 0\n",
    "w = 0\n",
    "for tweet in tweets:\n",
    "    tokens  = nlp(tweet)\n",
    "    for sentence in tokens.sents:\n",
    "        if len(sentence) > 4:\n",
    "            for j, word in enumerate(sentence):\n",
    "                w += 1\n",
    "                if np.sum(word.vector) == 0:\n",
    "                    z += 1\n",
    "#             if j < max_len:\n",
    "#                 tweet_x[i][j] = word.vector\n",
    "        \n",
    "            i += 1\n",
    "print(i / tweet_sen)\n",
    "print(z / w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_x = np.zeros((blog_sen, max_len, vec_size))\n",
    "i = 0\n",
    "for blog in blogs:\n",
    "    tokens  = nlp(blog)\n",
    "    for sentence in tokens.sents:\n",
    "        for j, word in enumerate(sentence):\n",
    "            if j < max_len:\n",
    "                blog_x[i][j] = word.vector\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_x = np.zeros((news_sen, max_len, vec_size))\n",
    "i = 0\n",
    "for new in news:\n",
    "    tokens  = nlp(new)\n",
    "    for sentence in tokens.sents:\n",
    "        for j, word in enumerate(sentence):\n",
    "            if j < max_len:\n",
    "                news_x[i][j] = word.vector\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_y = np.repeat(0, blog_sen)\n",
    "blog_y = np.repeat(1, blog_sen)\n",
    "news_y = np.repeat(2, news_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.vstack([news_x, blog_x])\n",
    "labels = np.hstack([news_y, blog_y])\n",
    "labels = labels.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = to_categorical(labels, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(data, one_hot_labels, test_size=0.1, random_state=42)\n",
    "print('training size:', train_x.shape[0])\n",
    "print('testing size:', test_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(max_len, vec_size))) # returns a sequence of vectors of dimension 32\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True))) # returns a sequence of vectors of dimension 32\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(256)) # return a single vector of dimension 32\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001]\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "grid = {}\n",
    "i = 1\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "        \n",
    "        print('\\nTraining model with learning rate {} and decay {}...'.format(learning_rate, decay))\n",
    "\n",
    "        optimizer = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay, amsgrad=True)\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        history = model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, validation_data=(test_x, test_y))\n",
    "\n",
    "        grid['model{}_lr{}_dc{}'.format(i, learning_rate, decay)] = history\n",
    "        \n",
    "        # summarize history for accuracy\n",
    "        print('\\nAccuracy Plot')\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        # summarize history for loss\n",
    "        print('\\nLoss Plot')\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(grid.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid['model1_lr0.001_dc0.0'].history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid['model1_lr0.001_dc0.0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
